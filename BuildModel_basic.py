# ============================================================
# BuildModel_basic.py â€” ê¸°ì¡´ êµ¬ì¡° ìœ ì§€ + BN freeze ì¶”ê°€ ë²„ì „
# ============================================================

import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import (
    Dense, Dropout, BatchNormalization,
    TimeDistributed, GlobalAveragePooling2D, LSTM
)
from tensorflow.keras.optimizers import Adam, RMSprop


# ============================================================
# ðŸ”¥ Fine-tuning Callback (ê¸°ì¡´ ìœ ì§€)
# ============================================================
class FineTuneCallback(tf.keras.callbacks.Callback):
    def __init__(self, base_cnn, unfreeze_epoch=5, unfreeze_layers=30):
        super().__init__()
        self.base_cnn = base_cnn
        self.unfreeze_epoch = unfreeze_epoch
        self.unfreeze_layers = unfreeze_layers
        self.unfrozen = False

    def on_epoch_begin(self, epoch, logs=None):
        if (not self.unfrozen) and (epoch >= self.unfreeze_epoch):
            print(f"\nðŸ”¥ Fine-tuning ì‹œìž‘: CNN ë§ˆì§€ë§‰ {self.unfreeze_layers}ê°œ ë ˆì´ì–´ unfreeze\n")
            for layer in self.base_cnn.layers[-self.unfreeze_layers:]:
                layer.trainable = True
            self.unfrozen = True

            lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))
            new_lr = lr * 0.1
            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)
            print(f"âš™ Learning rate {lr} -> {new_lr}")


# ============================================================
# ðŸ”¥ build() â€” ìˆ˜ì • í¬ì¸íŠ¸ ìµœì†Œí™”
# ============================================================
def build(size, seq_len, learning_rate,
          optimizer_class,
          initial_weights,
          cnn_class,
          pre_weights,
          lstm_conf,
          cnn_train_type,
          dropout,
          classes):

    input_layer = Input(shape=(seq_len, size, size, 3))

    # CNN ë°±ë³¸ ìƒì„± (ê¸°ì¡´ ìœ ì§€)
    base_cnn = cnn_class(
        include_top=False,
        weights=pre_weights,
        input_shape=(size, size, 3)
    )

    finetune_cb = None

    # ============================================================
    # ðŸ”¥ (ì¶”ê°€ë¨) BatchNormalization freeze (BN drift ë°©ì§€)
    # ============================================================
    for layer in base_cnn.layers:
        if isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = False

    # ============================================================
    # CNN Train-Type ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
    # ============================================================
    if cnn_train_type == "static":
        base_cnn.trainable = False

    elif cnn_train_type == "retrain":
        base_cnn.trainable = True

    elif cnn_train_type == "static_finetune":
        base_cnn.trainable = False
        finetune_cb = FineTuneCallback(base_cnn, unfreeze_epoch=5, unfreeze_layers=30)

    else:
        print(f"[WARN] Unknown cnn_train_type='{cnn_train_type}', fallback to static")
        base_cnn.trainable = False

    # ============================================================
    # TimeDistributed CNN + GAP (ê¸°ì¡´ ìœ ì§€)
    # ============================================================
    x = TimeDistributed(base_cnn)(input_layer)
    x = TimeDistributed(GlobalAveragePooling2D())(x)

    # ============================================================
    # LSTM (ê¸°ì¡´ ìœ ì§€)
    # ============================================================
    lstm_cls, lstm_kwargs = lstm_conf
    if "units" not in lstm_kwargs:
        lstm_kwargs = {**lstm_kwargs, "units": 128}
    x = lstm_cls(**lstm_kwargs)(x)

    # ============================================================
    # Dense Head (ê¸°ì¡´ ìœ ì§€)
    # ============================================================
    x = BatchNormalization()(x)
    x = Dropout(dropout)(x)
    x = Dense(256, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(64, activation="relu")(x)

    if classes > 1:
        activation = "softmax"
        loss_func = "categorical_crossentropy"
    else:
        activation = "sigmoid"
        loss_func = "binary_crossentropy"

    predictions = Dense(classes, activation=activation)(x)

    OptimClass, opt_kwargs = optimizer_class
    optimizer = OptimClass(learning_rate=learning_rate, **opt_kwargs)

    model = Model(inputs=input_layer, outputs=predictions)
    model.compile(optimizer=optimizer, loss=loss_func, metrics=["accuracy"])

    print(model.summary())
    return model, finetune_cb
